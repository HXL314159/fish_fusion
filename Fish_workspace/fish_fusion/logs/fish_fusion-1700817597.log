 2023-11-24 17:20:00,913 - INFO - {'Exp_name': 'fish_fusion', 'Training': {'Batch_size': 128, 'Max_epoch': 300, 'learning_rate': 0.001, 'seed': 25, 'classes_num': 4}, 'Workspace': 'Fish_workspace'}
 2023-11-24 17:20:00,913 - INFO - DataParallel(
  (module): fish_fusion(
    (cnn6): Cnn6(
      (spec_augmenter): SpecAugmentation(
        (time_dropper): DropStripes()
        (freq_dropper): DropStripes()
      )
      (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_block1): ConvBlock5x5(
        (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv_block2): ConvBlock5x5(
        (conv1): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv_block3): ConvBlock5x5(
        (conv1): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (conv_block4): ConvBlock5x5(
        (conv1): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (fc1): Linear(in_features=512, out_features=512, bias=True)
    )
    (MobilenetV2): MobileNetV2(
      (features): Sequential(
        (0): Sequential(
          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (3): ReLU6(inplace=True)
        )
        (1): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
            (1): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): ReLU6(inplace=True)
            (4): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (4): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
            (4): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (5): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (4): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (8): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (9): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (10): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (11): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (4): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (12): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (13): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (14): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (15): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (16): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (17): InvertedResidual(
          (conv): Sequential(
            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU6(inplace=True)
            (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
            (4): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (5): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (6): ReLU6(inplace=True)
            (7): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (8): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (18): Sequential(
          (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
      )
      (fc1): Linear(in_features=1280, out_features=512, bias=True)
    )
    (resnet18): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (fc1): Linear(in_features=1536, out_features=512, bias=True)
    (classifer): Linear(in_features=512, out_features=4, bias=True)
  )
)
 2023-11-24 17:20:00,917 - INFO - Training dataloader: 5248 samples
 2023-11-24 17:20:00,917 - INFO - Val dataloader: 896 samples
 2023-11-24 17:20:00,917 - INFO - Test dataloader: 896 samples
 2023-11-24 17:20:00,917 - INFO - Starting new training run
 2023-11-24 17:21:48,291 - INFO - Training loss 0.6628982163057094 at epoch 0
 2023-11-24 17:22:11,088 - INFO - val_best_acc: 0.5675, best_epoch: 0
 2023-11-24 17:23:59,806 - INFO - Training loss 0.19163044851000716 at epoch 1
 2023-11-24 17:23:59,807 - INFO - val_best_acc: 0.5675, best_epoch: 0
 2023-11-24 17:25:48,155 - INFO - Training loss 0.16537782422653058 at epoch 2
 2023-11-24 17:25:48,156 - INFO - val_best_acc: 0.5675, best_epoch: 0
 2023-11-24 17:27:36,584 - INFO - Training loss 0.13350176811218262 at epoch 3
 2023-11-24 17:27:36,585 - INFO - val_best_acc: 0.5675, best_epoch: 0
 2023-11-24 17:29:25,805 - INFO - Training loss 0.10201522621621446 at epoch 4
 2023-11-24 17:29:25,805 - INFO - val_best_acc: 0.5675, best_epoch: 0
 2023-11-24 17:31:13,867 - INFO - Training loss 0.09377347705204313 at epoch 5
 2023-11-24 17:31:13,867 - INFO - val_best_acc: 0.5675, best_epoch: 0
 2023-11-24 17:33:02,346 - INFO - Training loss 0.06853502834352052 at epoch 6
 2023-11-24 17:33:02,346 - INFO - val_best_acc: 0.5675, best_epoch: 0
 2023-11-24 17:34:52,724 - INFO - Training loss 0.07175935473202205 at epoch 7
 2023-11-24 17:34:52,724 - INFO - val_best_acc: 0.5675, best_epoch: 0
 2023-11-24 17:36:40,630 - INFO - Training loss 0.07464383115492217 at epoch 8
 2023-11-24 17:36:40,631 - INFO - val_best_acc: 0.5675, best_epoch: 0
 2023-11-24 17:38:30,723 - INFO - Training loss 0.06978366778391164 at epoch 9
 2023-11-24 17:38:30,724 - INFO - val_best_acc: 0.5675, best_epoch: 0
 2023-11-24 17:40:19,078 - INFO - Training loss 0.05477675002795167 at epoch 10
 2023-11-24 17:40:41,414 - INFO - val_best_acc: 0.9625, best_epoch: 10
 2023-11-24 17:42:26,007 - INFO - Training loss 0.059666473600195676 at epoch 11
 2023-11-24 17:42:26,007 - INFO - val_best_acc: 0.9625, best_epoch: 10
 2023-11-24 17:44:11,637 - INFO - Training loss 0.043441340344299265 at epoch 12
 2023-11-24 17:44:11,637 - INFO - val_best_acc: 0.9625, best_epoch: 10
 2023-11-24 17:45:55,433 - INFO - Training loss 0.036266472678994986 at epoch 13
 2023-11-24 17:45:55,434 - INFO - val_best_acc: 0.9625, best_epoch: 10
 2023-11-24 17:47:39,475 - INFO - Training loss 0.041155920000519694 at epoch 14
 2023-11-24 17:47:39,476 - INFO - val_best_acc: 0.9625, best_epoch: 10
 2023-11-24 17:49:24,383 - INFO - Training loss 0.03859272106300767 at epoch 15
 2023-11-24 17:49:24,383 - INFO - val_best_acc: 0.9625, best_epoch: 10
 2023-11-24 17:51:11,077 - INFO - Training loss 0.03069548475238063 at epoch 16
 2023-11-24 17:51:11,078 - INFO - val_best_acc: 0.9625, best_epoch: 10
 2023-11-24 17:52:55,363 - INFO - Training loss 0.028187983033315437 at epoch 17
 2023-11-24 17:52:55,363 - INFO - val_best_acc: 0.9625, best_epoch: 10
 2023-11-24 17:54:39,691 - INFO - Training loss 0.03522957785895503 at epoch 18
 2023-11-24 17:54:39,691 - INFO - val_best_acc: 0.9625, best_epoch: 10
 2023-11-24 17:56:26,514 - INFO - Training loss 0.04618126571337443 at epoch 19
 2023-11-24 17:56:26,514 - INFO - val_best_acc: 0.9625, best_epoch: 10
 2023-11-24 17:58:10,482 - INFO - Training loss 0.03075942036514057 at epoch 20
 2023-11-24 17:58:33,328 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:00:19,990 - INFO - Training loss 0.032349585847951834 at epoch 21
 2023-11-24 18:00:19,991 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:02:04,552 - INFO - Training loss 0.021878073690459132 at epoch 22
 2023-11-24 18:02:04,553 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:03:49,266 - INFO - Training loss 0.019094931964035625 at epoch 23
 2023-11-24 18:03:49,266 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:05:34,398 - INFO - Training loss 0.020738433495663653 at epoch 24
 2023-11-24 18:05:34,398 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:07:18,259 - INFO - Training loss 0.05465161856036724 at epoch 25
 2023-11-24 18:07:18,259 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:09:03,205 - INFO - Training loss 0.05977667308208055 at epoch 26
 2023-11-24 18:09:03,205 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:10:49,413 - INFO - Training loss 0.04216002716069541 at epoch 27
 2023-11-24 18:10:49,413 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:12:33,797 - INFO - Training loss 0.032502157792517146 at epoch 28
 2023-11-24 18:12:33,798 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:14:18,736 - INFO - Training loss 0.0276787936255881 at epoch 29
 2023-11-24 18:14:18,737 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:16:03,169 - INFO - Training loss 0.05021031544452942 at epoch 30
 2023-11-24 18:16:25,429 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:18:17,039 - INFO - Training loss 0.028328634054604465 at epoch 31
 2023-11-24 18:18:17,039 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:20:00,473 - INFO - Training loss 0.013197253675415839 at epoch 32
 2023-11-24 18:20:00,473 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:21:45,066 - INFO - Training loss 0.009148761222604662 at epoch 33
 2023-11-24 18:21:45,067 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:23:29,547 - INFO - Training loss 0.021650080487455765 at epoch 34
 2023-11-24 18:23:29,547 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:25:13,479 - INFO - Training loss 0.02034685073460157 at epoch 35
 2023-11-24 18:25:13,480 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:26:57,905 - INFO - Training loss 0.03354855354523241 at epoch 36
 2023-11-24 18:26:57,906 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:28:41,891 - INFO - Training loss 0.017004663814654257 at epoch 37
 2023-11-24 18:28:41,891 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:30:25,939 - INFO - Training loss 0.013065039256882922 at epoch 38
 2023-11-24 18:30:25,940 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:32:09,894 - INFO - Training loss 0.020429040856159677 at epoch 39
 2023-11-24 18:32:09,894 - INFO - val_best_acc: 0.99125, best_epoch: 20
 2023-11-24 18:33:54,015 - INFO - Training loss 0.019065990212100852 at epoch 40
